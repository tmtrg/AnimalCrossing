{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Commun à tous les notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thuym\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Chargement les données\n",
    "data_path = 'turnips2.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calcul des profits pour chaque demi-journée\n",
    "columns = ['Mon-AM', 'Mon-PM', 'Tues-AM', 'Tues-PM', 'Wed-AM', 'Wed-PM', 'Thurs-AM', 'Thurs-PM', 'Fri-AM', 'Fri-PM', 'Sat-AM', 'Sat-PM']\n",
    "for col in columns:\n",
    "    data[col] = data[col] - data['Purchase']\n",
    "\n",
    "# Normalisation les données\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data[columns])\n",
    "\n",
    "# Clustering pour identifier les tendances\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(data_scaled)\n",
    "data['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que data_scaled est le DataFrame complet normalisé\n",
    "# Ajouter les étiquettes de cluster aux données\n",
    "data['cluster'] = kmeans.labels_\n",
    "\n",
    "# Pour chaque segment partiel, assurez-vous de conserver la structure des colonnes complète\n",
    "train_data_list = []\n",
    "for i in range(1, len(columns) + 1):  # Inclure tous les segments jusqu'à 'Sat-PM'\n",
    "    for index, row in data.iterrows():\n",
    "        # Créer un segment partiel avec des zéros pour les valeurs non encore observées\n",
    "        partial_row = [0] * len(columns)\n",
    "        partial_row[:i] = row[columns[:i]].values  # Copier les valeurs observées\n",
    "        train_data_list.append(partial_row + [row['cluster']])  # Ajouter la ligne au dataset\n",
    "\n",
    "# Convertir en DataFrame\n",
    "train_data = pd.DataFrame(train_data_list, columns=columns + ['cluster'])\n",
    "\n",
    "# Séparer les caractéristiques et les étiquettes\n",
    "X_partial = scaler.transform(train_data.drop('cluster', axis=1))  # Utiliser transform() directement\n",
    "y_partial = train_data['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en ensembles d'entraînement et de test pour les données partielles\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_partial, y_partial, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle de régression logistique\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thuym\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Chargement des données\n",
    "data_path = 'turnips2.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Calcul des profits pour chaque demi-journée\n",
    "columns = ['Mon-AM', 'Mon-PM', 'Tues-AM', 'Tues-PM', 'Wed-AM', 'Wed-PM', 'Thurs-AM', 'Thurs-PM', 'Fri-AM', 'Fri-PM', 'Sat-AM', 'Sat-PM']\n",
    "for col in columns:\n",
    "    data[col] = data[col] - data['Purchase']\n",
    "\n",
    "# Normalisation les données\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data[columns])\n",
    "\n",
    "# Clustering pour identifier les tendances\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "data['cluster'] = kmeans.fit_predict(data_scaled)\n",
    "\n",
    "# Préparation des données pour les trois premières demi-journées\n",
    "columns_3demi_journees = columns[:3]  # Mon-AM, Mon-PM, Tues-AM\n",
    "data_3demi_journees = data[columns_3demi_journees]\n",
    "\n",
    "# Normalisation des données pour les trois premières demi-journées\n",
    "scaler_3demi_journees = StandardScaler()\n",
    "data_scaled_3demi_journees = scaler_3demi_journees.fit_transform(data_3demi_journees)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled_3demi_journees, data['cluster'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle de régression logistique\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_pred = log_reg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Sauvegarde du modèle et du scaler pour utilisation future\n",
    "# Vous pouvez les sauvegarder avec joblib ou pickle, par exemple\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
